import tvm
from tvm import relay
from tvm.relay.dataflow_pattern import wildcard, is_op
from tvm.relay.build_module import bind_params_by_name

# This file forges the Python specification for the Apache TVM backend.
# Its purpose is to teach TVM how to offload 8x8 Conv2D operations
# to the "Wrench" (Gemmini) accelerator by generating the correct
# C-code calls to the hardware interface defined in "gemmini_rocc.h".

# --- 1. Define the Relay "Pattern Match" ---
# This pattern table identifies which operations in the computation graph
# should be offloaded to the Wrench accelerator.
# Per the spec, we only offload 8x8 INT8 matrix multiplies (represented as conv2d).

GEMMINI_PATTERN_TABLE = [
    (
        "gemmini.conv2d",
        is_op("nn.conv2d")(wildcard(), wildcard()).has_attr({"kernel_size": [8, 8], "data_layout": "NHWC"}),
    )
]

# --- 2. Define the Composite Function ---
# This function groups the matched pattern into a single function that
# will be replaced by our externally-defined C code.

@relay.op.register_composite("gemmini.conv2d")
def gemmini_conv2d_composite(attrs, inputs, out):
    """Defines the composite function for the Gemmini Conv2D offload."""
    # For this task, the composite function is just the conv2d operation itself.
    data = inputs[0]
    weight = inputs[1]
    conv2d = relay.nn.conv2d(data, weight, **attrs)
    return conv2d

# --- 3. Define the "Extern C" Code Generation (CRITICAL) ---
# This is the core of the specification. It generates a C function as a string
# that TVM will include in the final compiled artifact. This C function
# MUST call the macros from gemmini_rocc.h in the exact, non-negotiable sequence.

@tvm.register_func("relay.ext.gemmini")
def gemmini_codegen(node):
    """
    The code generator for the "Wrench" (Gemmini) backend.
    It receives a Relay expression 'node' and returns the C source code
    that implements it using the Gemmini RoCC instructions.
    """
    # From the Relay node, we can identify the pointers to the input, weight,
    # and output tensors. TVM's C backend will manage the actual memory,
    # and we refer to the buffers by their generated names.
    output_buffer = node.args[-1].name_hint
    pointer_to_inputs = node.args[0].name_hint
    pointer_to_weights = node.args[1].name_hint

    # The spec requires 'packed_dimensions' to be passed to the CONFIG command.
    # This would typically be calculated by the compiler based on tensor shapes.
    # For this spec, we will create a placeholder variable representing this.
    packed_dimensions_var = "packed_dimensions_from_compiler"

    # Forge the C-code string with the exact mandated sequence.
    # This string is the deliverable that TVM uses to talk to our hardware.
    code = f"""
/* This code (generated by TVM) MUST use the lotus_noc HAL (TRM v2.0 Sec 5.0) */
#include "gemmini_rocc.h"
#include "noc_hal.h" // (A new file we will create)

#ifdef __cplusplus
extern "C"
#endif
void {node.name_hint}(void* global_pointer_to_inputs, void* global_pointer_to_weights, void* global_pointer_to_output_buffer) {{
    // The compiler is expected to calculate the packed dimensions for the hardware.
    uint64_t {packed_dimensions_var} = 0; // Placeholder for spec compliance.

    // Golden Knowledge (TRM v2.0 Sec 6.0)
    #define WRENCH_L2_BASE 0x20000000

    // 1. Define L2 pointers
    void* l2_inputs = (void*)WRENCH_L2_BASE;
    void* l2_weights = (void*)(WRENCH_L2_BASE + 4096);
    void* l2_output = (void*)(WRENCH_L2_BASE + 8192);

    // 2. Copy Inputs/Weights from Global (GDDR6) to Wrench L2
    noc_copy_blocking(l2_inputs, global_pointer_to_inputs, 4096);
    noc_copy_blocking(l2_weights, global_pointer_to_weights, 4096);

    // 3. Call Wrench using L2 POINTERS (CONFIG FIRST)
    GEMMINI_CONFIG({packed_dimensions_var});
    GEMMINI_LOAD_A((uint64_t)l2_weights);
    GEMMINI_LOAD_B((uint64_t)l2_inputs);
    GEMMINI_FENCE(); // Load fence
    GEMMINI_EXECUTE((uint64_t)l2_output);
    GEMMINI_FENCE(); // Execute fence

    // 4. Copy Output from Wrench L2 back to Global (GDDR6)
    noc_copy_blocking(global_pointer_to_output_buffer, l2_output, 4096);
}}
"""
    # Return the C source code within a TVM SourceModule.
    return tvm.codegen.SourceModule(code, "c")

def partition_for_gemmini(mod, params=None):
    """
    This function demonstrates how the TVM compilation flow uses the
    patterns and codegen functions above to partition the graph.
    """
    if params:
        mod["main"] = bind_params_by_name(mod["main"], params)

    seq = tvm.transform.Sequential(
        [
            relay.transform.InferType(),
            relay.transform.MergeComposite(GEMMINI_PATTERN_TABLE),
            relay.transform.AnnotateTarget("gemmini"),
            relay.transform.PartitionGraph(),
            relay.transform.InferType(),
        ]
    )

    with tvm.transform.PassContext(opt_level=3):
        mod = seq(mod)
    return mod
